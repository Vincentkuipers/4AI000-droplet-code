The device that will be used in training is cpu
Epoch 1Training loss is 18.45110321044922Training loss is 7458.47607421875Training loss is 6167.0712890625Training loss is 1486.3970947265625Training loss is 1008.9798583984375Training loss is 3583.44140625Training loss is 3859.722900390625Training loss is 1015.23486328125Training loss is 633.9993286132812Training loss is 907.4821166992188Training loss is 955.4810791015625Training loss is 330.2040710449219Training loss is 495.4341735839844Training loss is 1377.337158203125Training loss is 1027.4039306640625Training loss is 168.38502502441406Training loss is 266.43035888671875Training loss is 443.2658996582031Training loss is 634.1182861328125Training loss is 129.12918090820312Training loss is 218.22970581054688Training loss is 462.4120788574219Training loss is 313.9979553222656Training loss is 73.29901885986328Training loss is 53.95001220703125Training loss is 203.3748321533203Training loss is 169.26564025878906Training loss is 31.678462982177734Training loss is 21.561491012573242Training loss is 315.9537048339844Training loss is 78.0234603881836Training loss is 61.409271240234375Training loss is 28.75469970703125Training loss is 195.88665771484375Training loss is 64.11996459960938Training loss is 291.3879699707031Training loss is 341.5709228515625Training loss is 67.45851135253906Training loss is 153.0390167236328Training loss is 506.3321838378906Training loss is 142.64755249023438Training loss is 65.84648132324219Training loss is 182.1563262939453Training loss is 117.92161560058594Training loss is 189.95787048339844Training loss is 482.9293518066406Training loss is 167.01522827148438Training loss is 44.2988166809082Training loss is 31.494972229003906Training loss is 27.28142547607422Training loss is 33.11141586303711Training loss is 7.339357376098633Training loss is 9.535405158996582Training loss is 42.93844223022461Training loss is 30.240137100219727Training loss is 70.35274505615234Training loss is 29.041852951049805Training loss is 31.007511138916016Training loss is 11.61582088470459Training loss is 12.876502990722656Training loss is 32.00313186645508Training loss is 207.69482421875Training loss is 392.21575927734375Training loss is 113.78021240234375Training loss is 101.13197326660156Training loss is 232.30615234375Training loss is 170.46481323242188Training loss is 95.34292602539062Training loss is 224.78289794921875Training loss is 84.70613861083984Training loss is 367.1008605957031Training loss is 621.026123046875Training loss is 146.001220703125Training loss is 31.637184143066406Training loss is 47.322265625Training loss is 22.66904067993164Training loss is 136.75448608398438Training loss is 179.7607421875Training loss is 85.70487213134766Training loss is 110.20864868164062Training loss is 225.32106018066406Training loss is 68.74615478515625Training loss is 74.93583679199219Training loss is 128.58775329589844Training loss is 18.52802276611328Training loss is 144.21063232421875Training loss is 52.96875762939453Training loss is 38.94135665893555Training loss is 118.01310729980469Training loss is 267.9922790527344Training loss is 77.5439224243164Training loss is 48.22032165527344Training loss is 84.04560852050781Training loss is 28.29098129272461Training loss is 135.01284790039062Training loss is 159.99783325195312Training loss is 73.05326843261719Training loss is 112.83281707763672Training loss is 69.32389068603516Training loss is 12.80246353149414Training loss is 137.0322265625Training loss is 108.28878021240234Training loss is 21.84364128112793Training loss is 144.58413696289062Training loss is 352.97601318359375Training loss is 153.46612548828125Training loss is 60.651493072509766Training loss is 164.02935791015625Training loss is 29.432514190673828Training loss is 134.7249755859375Training loss is 278.07855224609375Training loss is 239.2877197265625Training loss is 96.56742095947266Training loss is 107.76870727539062Training loss is 15.84782886505127Training loss is 90.83348083496094Training loss is 263.3465576171875Training loss is 94.79467010498047Training loss is 55.855751037597656Training loss is 108.16790771484375Training loss is 14.346001625061035Training loss is 71.54769134521484Training loss is 166.1209716796875Training loss is 6.32965087890625Training loss is 8.931957244873047Training loss is 31.01030921936035Training loss is 72.57600402832031Training loss is 90.34820556640625Training loss is 52.1700439453125Training loss is 14.980066299438477Training loss is 115.9460678100586Training loss is 217.36253356933594Training loss is 78.4022445678711Training loss is 76.91072082519531Training loss is 455.38629150390625Training loss is 394.4414978027344Training loss is 43.49040222167969Training loss is 162.03614807128906Training loss is 478.4629211425781Training loss is 493.0724182128906Training loss is 167.10426330566406Training loss is 122.5131607055664Training loss is 340.0888671875Training loss is 253.40151977539062Training loss is 23.378232955932617Training loss is 342.68792724609375Training loss is 599.5952758789062Training loss is 562.1744384765625Training loss is 71.07432556152344Training loss is 154.5092010498047Training loss is 612.486083984375Training loss is 661.7225341796875Training loss is 57.361454010009766Training loss is 140.20159912109375Training loss is 353.27239990234375Training loss is 166.96546936035156Training loss is 66.77062225341797Training loss is 227.06915283203125Training loss is 474.74261474609375Training loss is 399.6505126953125Training loss is 225.14059448242188Training loss is 200.25962829589844Training loss is 219.6836395263672Training loss is 152.960693359375Training loss is 14.112650871276855Training loss is 286.8497619628906Training loss is 386.8309326171875Training loss is 330.9570617675781Training loss is 27.136350631713867Training loss is 435.9411926269531Training loss is 647.6131591796875Training loss is 540.3480224609375Training loss is 129.2506561279297Training loss is 121.18692016601562Training loss is 591.6629638671875Training loss is 254.68862915039062Training loss is 20.641260147094727Training loss is 35.40107727050781Training loss is 97.51266479492188Training loss is 109.17440032958984Training loss is 13.343118667602539Training loss is 232.62252807617188Training loss is 555.6749267578125Training loss is 346.58184814453125Training loss is 75.19871520996094Training loss is 298.5016784667969Training loss is 669.5849609375Training loss is 764.791015625Training loss is 396.6883239746094Training loss is 35.60893630981445Training loss is 178.92752075195312Training loss is 141.21817016601562Training loss is 34.05046463012695Training loss is 11.046991348266602Training loss is 24.437606811523438Training loss is 6.604903697967529Training loss is 62.200340270996094Training loss is 20.63640594482422Training loss is 49.46641159057617Training loss is 82.51131439208984Training loss is 16.56381607055664Training loss is 16.098661422729492Training loss is 55.90278625488281Training loss is 139.15240478515625Training loss is 13.908760070800781Training loss is 134.7576904296875Training loss is 254.15716552734375Training loss is 137.47109985351562Training loss is 83.56697082519531Training loss is 289.16510009765625Training loss is 110.83483123779297Training loss is 76.53836822509766Training loss is 136.24180603027344Training loss is 43.018409729003906Training loss is 52.0011100769043Training loss is 118.7486343383789Training loss is 40.85485076904297Training loss is 71.24018096923828Training loss is 170.54498291015625Training loss is 31.222671508789062Traceback (most recent call last):
  File "/home/poly2/4AI000/Diederik/4AI000/Model/runmodel.py", line 42, in <module>
    trainer.fit(epochs=50, batch_size=BATCH_SIZE, continue_training=False)
  File "/home/poly2/4AI000/Diederik/4AI000/Model/Trainer.py", line 156, in fit
    metrics_val, _, _ = self.val_epoch(dl_val)
  File "/home/poly2/4AI000/Diederik/4AI000/Model/Trainer.py", line 112, in val_epoch
    out.append(output[i].cpu().numpy())
RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.
