{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "    \n",
    "# train_data = pd.read_csv(\"./savefolderpytorch/train.csv\")\n",
    "# val_data = pd.read_csv(\"./savefolderpytorch/val.csv\")\n",
    "\n",
    "\n",
    "# train_data_epoch = train_data.groupby('epoch')['loss'].mean()\n",
    "# val_data_epoch = val_data.groupby('epoch')['loss'].mean()\n",
    "\n",
    "# plt.plot(train_data_epoch.index, train_data_epoch.values)\n",
    "# plt.plot(val_data_epoch.index, val_data_epoch.values)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.legend(['Train', 'Validation'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MAIN_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"Solid_droplet\", \"Data\")\n",
    "\n",
    "from DataLoader import *\n",
    "from Trainer import *\n",
    "from Model import *\n",
    "from RESNET import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20192584\\Anaconda3\\envs\\AI000\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_path = './savefolderpytorch/model3_2.pt'\n",
    "\n",
    "# instantiate your model\n",
    "model = CNNModel3() \n",
    "\n",
    "# load your model. Here we're loading on CPU since we're not going to do \n",
    "# large amounts of inference\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu'))) \n",
    "\n",
    "# put it in evaluation mode for inference\n",
    "model.eval()\n",
    "\n",
    "model.conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines two global scope variables to store our gradients and activations\n",
    "gradients = None\n",
    "activations = None\n",
    "\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "  global gradients # refers to the variable in the global scope\n",
    "  print('Backward hook running...')\n",
    "  gradients = grad_output\n",
    "  # In this case, we expect it to be torch.Size([batch size, 1024, 8, 8])\n",
    "  print(f'Gradients size: {gradients[0].size()}') \n",
    "  # We need the 0 index because the tensor containing the gradients comes\n",
    "  # inside a one element tuple.\n",
    "\n",
    "def forward_hook(module, args, output):\n",
    "  global activations # refers to the variable in the global scope\n",
    "  print('Forward hook running...')\n",
    "  activations = output\n",
    "  # In this case, we expect it to be torch.Size([batch size, 1024, 8, 8])\n",
    "  print(f'Activations size: {activations.size()}')\n",
    "\n",
    "backward_hook = model.conv1.register_full_backward_hook(backward_hook)\n",
    "forward_hook = model.conv1.register_forward_hook(forward_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_path = '../Solid_droplet/Data/67.0_32.0-1.0-20.0-13-0.png'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "image_size = 256\n",
    "transform = transforms.Compose([\n",
    "\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ])\n",
    "\n",
    "img_tensor = transform(image) # stores the tensor that represents the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward hook running...\n",
      "Activations size: torch.Size([1, 8, 1076, 1916])\n",
      "Backward hook running...\n",
      "Gradients size: torch.Size([1, 8, 1076, 1916])\n"
     ]
    }
   ],
   "source": [
    "model(img_tensor.unsqueeze(0)).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool the gradients across the channels\n",
    "pooled_gradients = torch.mean(gradients[0], dim=[0, 2, 3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision import utils\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# weight the channels by corresponding gradients\n",
    "for i in range(activations.size()[1]):\n",
    "    activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "# average the channels of the activations\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "# relu on top of the heatmap\n",
    "heatmap = F.relu(heatmap)\n",
    "\n",
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "\n",
    "# # draw the heatmap\n",
    "# plt.matshow(heatmap.detach())\n",
    "\n",
    "# Convert the heatmap to a PIL image\n",
    "heatmap_normalized = torch.clamp(heatmap, 0, 1)  # Ensure values are between 0 and 1\n",
    "heatmap_pil = ToPILImage()(heatmap_normalized)\n",
    "\n",
    "# Save the heatmap as an image\n",
    "heatmap_pil.save('heatmap5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from torchvision.transforms.functional import to_pil_image\n",
    "# from matplotlib import colormaps\n",
    "# import numpy as np\n",
    "# import PIL\n",
    "\n",
    "# # Create a figure and plot the first image\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.axis('off') # removes the axis markers\n",
    "\n",
    "# # First plot the original image\n",
    "# ax.imshow(to_pil_image(img_tensor, mode='RGB'))\n",
    "\n",
    "# # Resize the heatmap to the same size as the input image and defines\n",
    "# # a resample algorithm for increasing image resolution\n",
    "# # we need heatmap.detach() because it can't be converted to numpy array while\n",
    "# # requiring gradients\n",
    "# overlay = to_pil_image(heatmap.detach(), mode='F').resize((256,256), resample=PIL.Image.BICUBIC)\n",
    "\n",
    "# # Apply any colormap you want\n",
    "# cmap = colormaps['jet']\n",
    "# overlay = (255 * cmap(np.asarray(overlay) ** 2)[:, :, :3]).astype(np.uint8)\n",
    "\n",
    "# # Plot the heatmap on the same axes, \n",
    "# # but with alpha < 1 (this defines the transparency of the heatmap)\n",
    "# ax.imshow(overlay, alpha=0.4, interpolation='nearest', extent=extent)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
